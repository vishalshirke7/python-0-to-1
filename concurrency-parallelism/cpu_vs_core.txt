A core is usually the basic computation unit of the CPU - it can run a single program context (or multiple ones if it supports hardware threads such as hyperthreading on Intel CPUs), maintaining the correct program state, registers, and correct execution order, and performing the operations through ALUs. For optimization purposes, a core can also hold on-core caches with copies of frequently used memory chunks.

A CPU may have one or more cores to perform tasks at a given time. These tasks are usually software processes and threads that the OS schedules. Note that the OS may have many threads to run, but the CPU can only run X such tasks at a given time, where X = number cores * number of hardware threads per core. The rest would have to wait for the OS to schedule them whether by preempting currently running tasks or any other means.

If you run one single-threaded application, it will be allocated to a single core, and activate only a single HW thread on it. That thread will be able to utilize 100% of that core as there is no other thread running along with it. If you activate another application it will usually be allocated to another core, and so on until you saturate the number of cores. Beyond that any additional thread will be scheduled to the same cores again using SMT. At that point, the core utilization will be distributed between the 2 threads (and if they behave symmetrically, each would likely reach ~50%)